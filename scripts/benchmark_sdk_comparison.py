#!/usr/bin/env python3
"""
Claude Agent SDKè·¨è¯­è¨€æ€§èƒ½å¯¹æ¯”æµ‹è¯•å·¥å…·

æ”¯æŒæµ‹è¯•Rustã€Pythonå’ŒNode.js SDKçš„æ€§èƒ½ï¼Œå¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šã€‚
"""

import asyncio
import json
import subprocess
import time
from dataclasses import dataclass
from typing import Dict, List, Optional
from pathlib import Path
import statistics


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    name: str
    mean_ms: float
    median_ms: float
    p95_ms: float
    p99_ms: float
    min_ms: float
    max_ms: float
    std_dev_ms: float
    samples: int


class SDKBenchmark:
    """SDKåŸºå‡†æµ‹è¯•å™¨"""

    def __init__(self, iterations: int = 50, timeout: int = 30):
        self.iterations = iterations
        self.timeout = timeout

    def _run_rust_example(self, example_name: str, prompt: str) -> float:
        """è¿è¡ŒRustç¤ºä¾‹å¹¶æµ‹é‡æ—¶é—´"""
        start = time.perf_counter()
        try:
            result = subprocess.run(
                ["cargo", "run", "--release", "--example", example_name],
                input=prompt.encode(),
                capture_output=True,
                timeout=self.timeout,
                cwd=Path(__file__).parent.parent
            )
            if result.returncode != 0:
                print(f"Rust error: {result.stderr.decode()}")
                return -1
        except subprocess.TimeoutExpired:
            return -1
        return (time.perf_counter() - start) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’

    def _run_python_sdk(self, prompt: str) -> float:
        """è¿è¡ŒPython SDKå¹¶æµ‹é‡æ—¶é—´"""
        try:
            from anthropic import Anthropic
            client = Anthropic()

            start = time.perf_counter()
            client.messages.create(
                model="claude-sonnet-4-5",
                max_tokens=1024,
                messages=[{"role": "user", "content": prompt}]
            )
            return (time.perf_counter() - start) * 1000
        except Exception as e:
            print(f"Python error: {e}")
            return -1

    def _run_nodejs_sdk(self, prompt: str) -> float:
        """è¿è¡ŒNode.js SDKå¹¶æµ‹é‡æ—¶é—´"""
        script = """
const anthropic = require('@anthropic-ai/sdk');
const client = new anthropic.Anthropic();

async function query(prompt) {
    const start = Date.now();
    await client.messages.create({
        model: 'claude-sonnet-4-5',
        max_tokens: 1024,
        messages: [{ role: 'user', content: prompt }]
    });
    return Date.now() - start;
}

query(process.argv[2]).then(time => console.log(time));
"""
        start = time.perf_counter()
        try:
            result = subprocess.run(
                ["node", "-e", script, prompt],
                capture_output=True,
                timeout=self.timeout
            )
            if result.returncode != 0:
                print(f"Node.js error: {result.stderr.decode()}")
                return -1
            return float(result.stdout.decode().strip())
        except subprocess.TimeoutExpired:
            return -1
        except ValueError:
            return -1
        return (time.perf_counter() - start) * 1000

    def benchmark_rust(self, prompt: str, example: str = "01_hello_world") -> BenchmarkResult:
        """è¿è¡ŒRust SDKåŸºå‡†æµ‹è¯•"""
        print(f"è¿è¡ŒRust SDKæµ‹è¯• ({self.iterations}æ¬¡è¿­ä»£)...")
        times: List[float] = []

        for i in range(self.iterations):
            elapsed = self._run_rust_example(example, prompt)
            if elapsed > 0:
                times.append(elapsed)
            print(f"  è¿­ä»£ {i+1}/{self.iterations}: {elapsed:.1f}ms", end='\r')

        if not times:
            raise RuntimeError("Rust SDKæµ‹è¯•å¤±è´¥: æ‰€æœ‰è¿­ä»£éƒ½è¶…æ—¶æˆ–å‡ºé”™")

        return self._calculate_statistics("Rust SDK", times)

    def benchmark_python(self, prompt: str) -> BenchmarkResult:
        """è¿è¡ŒPython SDKåŸºå‡†æµ‹è¯•"""
        print(f"è¿è¡ŒPython SDKæµ‹è¯• ({self.iterations}æ¬¡è¿­ä»£)...")
        times: List[float] = []

        for i in range(self.iterations):
            elapsed = self._run_python_sdk(prompt)
            if elapsed > 0:
                times.append(elapsed)
            print(f"  è¿­ä»£ {i+1}/{self.iterations}: {elapsed:.1f}ms", end='\r')

        if not times:
            raise RuntimeError("Python SDKæµ‹è¯•å¤±è´¥: æ‰€æœ‰è¿­ä»£éƒ½è¶…æ—¶æˆ–å‡ºé”™")

        return self._calculate_statistics("Python SDK", times)

    def benchmark_nodejs(self, prompt: str) -> BenchmarkResult:
        """è¿è¡ŒNode.js SDKåŸºå‡†æµ‹è¯•"""
        print(f"è¿è¡ŒNode.js SDKæµ‹è¯• ({self.iterations}æ¬¡è¿­ä»£)...")
        times: List[float] = []

        for i in range(self.iterations):
            elapsed = self._run_nodejs_sdk(prompt)
            if elapsed > 0:
                times.append(elapsed)
            print(f"  è¿­ä»£ {i+1}/{self.iterations}: {elapsed:.1f}ms", end='\r')

        if not times:
            raise RuntimeError("Node.js SDKæµ‹è¯•å¤±è´¥: æ‰€æœ‰è¿­ä»£éƒ½è¶…æ—¶æˆ–å‡ºé”™")

        return self._calculate_statistics("Node.js SDK", times)

    def _calculate_statistics(self, name: str, times: List[float]) -> BenchmarkResult:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        sorted_times = sorted(times)
        n = len(times)

        return BenchmarkResult(
            name=name,
            mean_ms=statistics.mean(times),
            median_ms=statistics.median(times),
            p95_ms=sorted_times[int(n * 0.95)] if n >= 20 else max(times),
            p99_ms=sorted_times[int(n * 0.99)] if n >= 100 else max(times),
            min_ms=min(times),
            max_ms=max(times),
            std_dev_ms=statistics.stdev(times) if n > 1 else 0,
            samples=n
        )

    def print_comparison_table(self, results: Dict[str, BenchmarkResult]):
        """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""
        print("\n" + "="*100)
        print("æ€§èƒ½å¯¹æ¯”ç»“æœ")
        print("="*100)

        # æ‰“å°è¡¨å¤´
        print(f"{'åœºæ™¯':<20} {'SDK':<15} {'å¹³å‡':<10} {'ä¸­ä½æ•°':<10} {'P95':<10} {'P99':<10} {'æ ‡å‡†å·®':<10}")
        print("-" * 100)

        # æ‰“å°æ¯ä¸ªåœºæ™¯çš„ç»“æœ
        for scenario, results_dict in results.items():
            for sdk_name, result in results_dict.items():
                print(f"{scenario:<20} {sdk_name:<15} "
                      f"{result.mean_ms:<10.1f} "
                      f"{result.median_ms:<10.1f} "
                      f"{result.p95_ms:<10.1f} "
                      f"{result.p99_ms:<10.1f} "
                      f"{result.std_dev_ms:<10.1f}")
            print()

    def generate_markdown_report(self, results: Dict[str, BenchmarkResult], output_path: str = "benchmark_results.md"):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        report = []
        report.append("# Claude Agent SDK æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š\n")
        report.append(f"**ç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        report.append(f"**æµ‹è¯•é…ç½®**: æ¯ä¸ªSDK {self.iterations} æ¬¡è¿­ä»£\n")

        # æ¦‚è§ˆè¡¨æ ¼
        report.append("## æ€§èƒ½æ¦‚è§ˆ\n")
        report.append("| åœºæ™¯ | SDK | å¹³å‡ (ms) | ä¸­ä½æ•° (ms) | P95 (ms) | P99 (ms) | æ ‡å‡†å·® (ms) |")
        report.append("|------|-----|-----------|-------------|----------|----------|-------------|")

        for scenario, results_dict in results.items():
            for sdk_name, result in results_dict.items():
                report.append(
                    f"| {scenario} | {sdk_name} | "
                    f"{result.mean_ms:.1f} | "
                    f"{result.median_ms:.1f} | "
                    f"{result.p95_ms:.1f} | "
                    f"{result.p99_ms:.1f} | "
                    f"{result.std_dev_ms:.1f} |"
                )

        # è¯¦ç»†åˆ†æ
        report.append("\n## è¯¦ç»†åˆ†æ\n")

        for scenario, results_dict in results.items():
            report.append(f"### {scenario}\n")

            # æ‰¾å‡ºæœ€å¿«çš„SDK
            fastest = min(results_dict.values(), key=lambda r: r.mean_ms)

            for sdk_name, result in results_dict.items():
                speedup = result.mean_ms / fastest.mean_ms
                report.append(f"#### {sdk_name}\n")
                report.append(f"- å¹³å‡å»¶è¿Ÿ: **{result.mean_ms:.1f}ms**")
                report.append(f"- ç›¸å¯¹æ€§èƒ½: {speedup:.2f}x " +
                             ("(æœ€å¿«) ğŸš€" if speedup == 1.0 else f"({speedup:.2f}x æ…¢)"))
                report.append(f"- å»¶è¿ŸèŒƒå›´: {result.min_ms:.1f}ms - {result.max_ms:.1f}ms")
                report.append(f"- æ ‡å‡†å·®: {result.std_dev_ms:.1f}ms ({(result.std_dev_ms/result.mean_ms*100):.1f}% å˜å¼‚ç³»æ•°)")
                report.append("")

        # å»ºè®®
        report.append("## æ€§èƒ½å»ºè®®\n")

        for scenario, results_dict in results.items():
            report.append(f"### {scenario}\n")
            fastest_sdk = min(results_dict.items(), key=lambda x: x[1].mean_ms)
            report.append(f"- **æ¨è**: {fastest_sdk[0]} ({fastest_sdk[1].mean_ms:.1f}ms å¹³å‡å»¶è¿Ÿ)")
            report.append(f"- **æœ€æ…¢**: {max(results_dict.items(), key=lambda x: x[1].mean_ms)[0]}")

            # æ€§èƒ½å·®å¼‚åˆ†æ
            speeds = [r.mean_ms for r in results_dict.values()]
            variation = (max(speeds) - min(speeds)) / min(speeds) * 100
            report.append(f"- **æ€§èƒ½å·®å¼‚**: {variation:.1f}%")
            report.append("")

        # å†™å…¥æ–‡ä»¶
        output_file = Path(output_path)
        output_file.write_text("\n".join(report), encoding='utf-8')
        print(f"\næŠ¥å‘Šå·²ç”Ÿæˆ: {output_file.absolute()}")


async def main():
    """ä¸»å‡½æ•°"""
    print("Claude Agent SDK æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 50)

    # æµ‹è¯•åœºæ™¯
    test_scenarios = {
        "ç®€å•æŸ¥è¯¢": "What is 2 + 2?",
        "ä¸­ç­‰å¤æ‚åº¦": "Explain the concept of recursion in programming",
        "ä»£ç ç”Ÿæˆ": "Write a function to calculate fibonacci numbers in Python",
    }

    benchmark = SDKBenchmark(iterations=30, timeout=60)
    all_results = {}

    for scenario_name, prompt in test_scenarios.items():
        print(f"\n{'='*50}")
        print(f"æµ‹è¯•åœºæ™¯: {scenario_name}")
        print(f"Prompt: {prompt[:50]}...")
        print(f"{'='*50}\n")

        scenario_results = {}

        # æµ‹è¯•æ¯ä¸ªSDK
        try:
            scenario_results["Rust"] = benchmark.benchmark_rust(prompt, "01_hello_world")
        except Exception as e:
            print(f"Rust SDKæµ‹è¯•å¤±è´¥: {e}")

        try:
            scenario_results["Python"] = benchmark.benchmark_python(prompt)
        except Exception as e:
            print(f"Python SDKæµ‹è¯•å¤±è´¥: {e}")

        try:
            scenario_results["Node.js"] = benchmark.benchmark_nodejs(prompt)
        except Exception as e:
            print(f"Node.js SDKæµ‹è¯•å¤±è´¥: {e}")

        all_results[scenario_name] = scenario_results

    # æ‰“å°ç»“æœ
    benchmark.print_comparison_table(all_results)

    # ç”ŸæˆæŠ¥å‘Š
    benchmark.generate_markdown_report(all_results)


if __name__ == "__main__":
    asyncio.run(main())
