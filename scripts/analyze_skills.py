#!/usr/bin/env python3
"""
SKILL.md æ·±åº¦åˆ†æå’Œæ•ˆæœéªŒè¯è„šæœ¬

ä¸ä»…éªŒè¯æ–‡ä»¶æ ¼å¼ï¼Œè¿˜åˆ†æï¼š
1. å†…å®¹è´¨é‡å’Œæ·±åº¦
2. ä»£ç ç¤ºä¾‹çš„å®Œæ•´æ€§
3. æŠ€æœ¯è¦†ç›–çš„å¹¿åº¦
4. å®ç”¨æ€§è¯„åˆ†
5. å­¦ä¹ è·¯å¾„å®Œæ•´æ€§
"""

import os
import re
from pathlib import Path
from typing import Dict, List, Tuple
from collections import Counter
import json


def analyze_code_examples(content: str) -> Dict:
    """åˆ†æä»£ç ç¤ºä¾‹"""
    # æ£€æµ‹ä»£ç å—
    code_blocks = re.findall(r'```[\w]*\n(.*?)\n```', content, re.DOTALL)

    # ç»Ÿè®¡ç¼–ç¨‹è¯­è¨€
    languages = re.findall(r'```(\w+)', content)

    # ç»Ÿè®¡ä»£ç è¡Œæ•°
    total_code_lines = sum(len(block.split('\n')) for block in code_blocks)

    # æ£€æµ‹ä»£ç è´¨é‡æŒ‡æ ‡
    has_comments = any('//' in block or '#' in block or '/*' in block
                       for block in code_blocks)
    has_error_handling = any('try' in block.lower() or 'catch' in block.lower()
                             or 'error' in block.lower() for block in code_blocks)
    has_best_practices = any(word in content.lower() for word in
                            ['best practice', 'æœ€ä½³å®è·µ', 'recommend', 'å»ºè®®'])

    return {
        'code_blocks': len(code_blocks),
        'languages': Counter(languages),
        'total_code_lines': total_code_lines,
        'has_comments': has_comments,
        'has_error_handling': has_error_handling,
        'has_best_practices': has_best_practices,
        'avg_code_lines_per_block': total_code_lines / len(code_blocks) if code_blocks else 0
    }


def analyze_content_quality(content: str) -> Dict:
    """åˆ†æå†…å®¹è´¨é‡"""
    # ç»Ÿè®¡ç« èŠ‚
    sections = re.findall(r'^#+\s+(.+)$', content, re.MULTILINE)

    # ç»Ÿè®¡åˆ—è¡¨é¡¹
    list_items = len(re.findall(r'^\s*[-*+]\s+', content, re.MULTILINE))

    # ç»Ÿè®¡è¡¨æ ¼
    tables = len(re.findall(r'\|.*\|', content))

    # æ£€æµ‹å…³é”®å†…å®¹
    has_introduction = any(word in content.lower() for word in
                          ['introduction', 'ä»‹ç»', 'overview', 'æ¦‚è¿°'])
    has_examples = any(word in content.lower() for word in
                      ['example', 'ç¤ºä¾‹', 'demo', 'æ¼”ç¤º'])
    has_best_practices = any(word in content.lower() for word in
                            ['best practice', 'æœ€ä½³å®è·µ', 'recommendation', 'å»ºè®®'])
    has_troubleshooting = any(word in content.lower() for word in
                             ['troubleshooting', 'æ•…éšœæ’é™¤', 'common issue', 'å¸¸è§é—®é¢˜'])
    has_tools = any(word in content.lower() for word in
                   ['tools', 'å·¥å…·', 'resources', 'èµ„æº'])

    # æ£€æµ‹ä¸­æ–‡å†…å®¹
    has_chinese = bool(re.search(r'[\u4e00-\u9fff]', content))

    # å†…å®¹æ·±åº¦æŒ‡æ ‡
    lines = content.split('\n')
    non_empty_lines = [l for l in lines if l.strip()]
    content_depth_score = len(non_empty_lines) / 100  # æ¯100è¡Œ1åˆ†

    return {
        'sections': len(sections),
        'list_items': list_items,
        'tables': tables,
        'has_introduction': has_introduction,
        'has_examples': has_examples,
        'has_best_practices': has_best_practices,
        'has_troubleshooting': has_troubleshooting,
        'has_tools': has_tools,
        'has_chinese': has_chinese,
        'content_depth_score': min(content_depth_score, 10),  # æœ€é«˜10åˆ†
        'non_empty_lines': len(non_empty_lines)
    }


def analyze_technical_coverage(content: str) -> Dict:
    """åˆ†ææŠ€æœ¯è¦†ç›–"""
    # æŠ€æœ¯å…³é”®è¯
    tech_keywords = {
        'languages': ['python', 'javascript', 'typescript', 'rust', 'go', 'java',
                     'swift', 'kotlin', 'ruby', 'php', 'c\+\+', 'c#'],
        'frameworks': ['react', 'vue', 'angular', 'django', 'flask', 'fastapi',
                      'spring', 'express', 'gin', 'echo', 'tensorflow', 'pytorch'],
        'databases': ['postgresql', 'mysql', 'mongodb', 'redis', 'elasticsearch',
                     'dynamodb', 'cassandra', 'neo4j'],
        'cloud': ['aws', 'azure', 'gcp', 'alibaba', 'terraform', 'kubernetes',
                 'docker', 'ansible', 'chef', 'puppet'],
        'tools': ['git', 'jenkins', 'github actions', 'gitlab ci', 'travis ci',
                 'prometheus', 'grafana', 'elk', 'jenkins'],
        'concepts': ['microservices', 'serverless', 'devops', 'cicd', 'tdd',
                    'bdd', 'agile', 'scrum', 'kubernetes', 'docker']
    }

    found_techs = {}
    content_lower = content.lower()

    for category, keywords in tech_keywords.items():
        found = []
        for keyword in keywords:
            if keyword.lower() in content_lower:
                found.append(keyword)
        if found:
            found_techs[category] = found

    return found_techs


def calculate_utility_score(quality: Dict, code: Dict, tech: Dict) -> float:
    """è®¡ç®—å®ç”¨æ€§è¯„åˆ† (0-100)"""
    score = 0

    # å†…å®¹è´¨é‡ (40åˆ†)
    if quality['has_introduction']:
        score += 5
    if quality['has_examples']:
        score += 10
    if quality['has_best_practices']:
        score += 10
    if quality['has_troubleshooting']:
        score += 5
    if quality['has_tools']:
        score += 5
    score += min(quality['content_depth_score'] / 2, 5)  # æœ€å¤š5åˆ†

    # ä»£ç è´¨é‡ (40åˆ†)
    if code['code_blocks'] > 0:
        score += 10
        score += min(code['code_blocks'] * 2, 10)  # æœ€å¤š10åˆ†
    if code['has_comments']:
        score += 10
    if code['has_error_handling']:
        score += 5
    if code['total_code_lines'] > 100:
        score += 5

    # æŠ€æœ¯è¦†ç›– (20åˆ†)
    tech_categories = len(tech)
    score += min(tech_categories * 4, 20)  # æœ€å¤š20åˆ†

    return min(score, 100)


def analyze_learning_path(skills: List[Dict]) -> Dict:
    """åˆ†æå­¦ä¹ è·¯å¾„å®Œæ•´æ€§"""
    # å®šä¹‰æŠ€èƒ½å±‚çº§
    skill_levels = {
        'beginner': ['example-calculator', 'api-tester', 'code-reviewer'],
        'intermediate': ['database-migrator', 'git-workflow', 'docker-helper',
                        'deployment-automation'],
        'advanced': ['frontend-developer', 'backend-developer', 'mobile-developer',
                    'performance-optimizer', 'logging-monitoring'],
        'expert': ['cloud-infrastructure', 'data-engineering',
                  'machine-learning-engineer', 'devops-engineer',
                  'security-auditor']
    }

    # å®šä¹‰é¢†åŸŸè·¯å¾„
    domain_paths = {
        'fullstack': ['frontend-developer', 'backend-developer', 'database-migrator'],
        'devops': ['docker-helper', 'git-workflow', 'deployment-automation',
                  'devops-engineer'],
        'data': ['data-engineering', 'machine-learning-engineer', 'performance-optimizer'],
        'mobile': ['mobile-developer', 'api-tester'],
        'cloud': ['cloud-infrastructure', 'devops-engineer', 'security-auditor']
    }

    skill_names = {s['metadata']['name'].lower().replace(' ', '-'): s
                   for s in skills}

    # æ£€æŸ¥æ¯æ¡è·¯å¾„çš„å®Œæ•´æ€§
    path_completion = {}
    for path_name, required_skills in domain_paths.items():
        completed = sum(1 for skill in required_skills if skill in skill_names)
        path_completion[path_name] = {
            'required': len(required_skills),
            'completed': completed,
            'percentage': (completed / len(required_skills)) * 100
        }

    return path_completion


def analyze_skill(skill_dir: Path) -> Dict:
    """æ·±åº¦åˆ†æå•ä¸ªæŠ€èƒ½"""
    skill_md = skill_dir / "SKILL.md"

    if not skill_md.exists():
        return None

    with open(skill_md, 'r', encoding='utf-8') as f:
        content = f.read()

    # è§£æ frontmatter
    metadata = {}
    in_frontmatter = False
    yaml_lines = []

    for line in content.split('\n'):
        if line.strip() == '---':
            if not in_frontmatter:
                in_frontmatter = True
            else:
                break
        elif in_frontmatter:
            yaml_lines.append(line)

    # ç®€å•è§£æ YAML
    for line in yaml_lines:
        if ':' in line:
            key, value = line.split(':', 1)
            metadata[key.strip()] = value.strip()

    # æå– markdown å†…å®¹
    markdown_start = content.find('---', 1)
    if markdown_start != -1:
        markdown_start = content.find('---', markdown_start + 3) + 3
        markdown_content = content[markdown_start:]
    else:
        markdown_content = content

    # æ·±åº¦åˆ†æ
    code_analysis = analyze_code_examples(markdown_content)
    quality_analysis = analyze_content_quality(markdown_content)
    tech_analysis = analyze_technical_coverage(markdown_content)
    utility_score = calculate_utility_score(quality_analysis, code_analysis, tech_analysis)

    return {
        'path': skill_dir.name,
        'metadata': metadata,
        'code_analysis': code_analysis,
        'quality_analysis': quality_analysis,
        'tech_analysis': tech_analysis,
        'utility_score': utility_score
    }


def print_analysis_report(skills: List[Dict]):
    """æ‰“å°æ·±åº¦åˆ†ææŠ¥å‘Š"""

    print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘           ğŸ¯ SKILL.md æ·±åº¦æ•ˆæœåˆ†ææŠ¥å‘Š                          â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    # æ€»ä½“ç»Ÿè®¡
    print("\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"   åˆ†ææŠ€èƒ½æ•°: {len(skills)} ä¸ª")

    avg_utility = sum(s['utility_score'] for s in skills) / len(skills)
    print(f"   å¹³å‡å®ç”¨æ€§è¯„åˆ†: {avg_utility:.1f}/100")

    total_code_blocks = sum(s['code_analysis']['code_blocks'] for s in skills)
    print(f"   æ€»ä»£ç å—æ•°: {total_code_blocks} ä¸ª")

    total_code_lines = sum(s['code_analysis']['total_code_lines'] for s in skills)
    print(f"   æ€»ä»£ç è¡Œæ•°: {total_code_lines} è¡Œ")

    # å®ç”¨æ€§æ’å
    print("\nğŸ† å®ç”¨æ€§æ’å (Top 10):")
    sorted_skills = sorted(skills, key=lambda x: x['utility_score'], reverse=True)[:10]

    for i, skill in enumerate(sorted_skills, 1):
        name = skill['metadata'].get('name', 'Unknown')
        score = skill['utility_score']
        emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
        print(f"   {emoji} {i}. {name}")
        print(f"      è¯„åˆ†: {score:.1f}/100")

        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        code_blocks = skill['code_analysis']['code_blocks']
        code_lines = skill['code_analysis']['total_code_lines']
        has_chinese = skill['quality_analysis']['has_chinese']

        print(f"      ä»£ç : {code_blocks} ä¸ªä»£ç å—, {code_lines} è¡Œ")
        print(f"      è¯­è¨€: {'ä¸­æ–‡' if has_chinese else 'è‹±æ–‡'}")
        print()

    # å†…å®¹è´¨é‡åˆ†æ
    print("ğŸ“ˆ å†…å®¹è´¨é‡åˆ†æ:")

    has_intro = sum(1 for s in skills if s['quality_analysis']['has_introduction'])
    has_examples = sum(1 for s in skills if s['quality_analysis']['has_examples'])
    has_best_practices = sum(1 for s in skills if s['quality_analysis']['has_best_practices'])
    has_troubleshooting = sum(1 for s in skills if s['quality_analysis']['has_troubleshooting'])
    has_tools = sum(1 for s in skills if s['quality_analysis']['has_tools'])

    print(f"   âœ… æœ‰ä»‹ç»: {has_intro}/{len(skills)} ({has_intro*100//len(skills)}%)")
    print(f"   âœ… æœ‰ç¤ºä¾‹: {has_examples}/{len(skills)} ({has_examples*100//len(skills)}%)")
    print(f"   âœ… æœ‰æœ€ä½³å®è·µ: {has_best_practices}/{len(skills)} ({has_best_practices*100//len(skills)}%)")
    print(f"   âœ… æœ‰æ•…éšœæ’é™¤: {has_troubleshooting}/{len(skills)} ({has_troubleshooting*100//len(skills)}%)")
    print(f"   âœ… æœ‰å·¥å…·èµ„æº: {has_tools}/{len(skills)} ({has_tools*100//len(skills)}%)")

    # ä»£ç è´¨é‡åˆ†æ
    print("\nğŸ’» ä»£ç è´¨é‡åˆ†æ:")

    has_comments = sum(1 for s in skills if s['code_analysis']['has_comments'])
    has_error_handling = sum(1 for s in skills if s['code_analysis']['has_error_handling'])

    print(f"   âœ… æœ‰æ³¨é‡Š: {has_comments}/{len(skills)} ({has_comments*100//len(skills)}%)")
    print(f"   âœ… æœ‰é”™è¯¯å¤„ç†: {has_error_handling}/{len(skills)} ({has_error_handling*100//len(skills)}%)")

    avg_code_per_block = sum(s['code_analysis']['avg_code_lines_per_block']
                            for s in skills) / len(skills)
    print(f"   ğŸ“Š å¹³å‡ä»£ç å—å¤§å°: {avg_code_per_block:.1f} è¡Œ")

    # æŠ€æœ¯è¦†ç›–åˆ†æ
    print("\nğŸ”§ æŠ€æœ¯è¦†ç›–åˆ†æ:")

    all_techs = {}
    for skill in skills:
        for category, techs in skill['tech_analysis'].items():
            if category not in all_techs:
                all_techs[category] = Counter()
            for tech in techs:
                all_techs[category][tech] += 1

    for category, techs in sorted(all_techs.items()):
        print(f"   {category}:")
        for tech, count in techs.most_common(5):
            print(f"      - {tech}: {count} ä¸ªæŠ€èƒ½")

    # å­¦ä¹ è·¯å¾„åˆ†æ
    print("\nğŸ“ å­¦ä¹ è·¯å¾„å®Œæ•´æ€§:")
    path_completion = analyze_learning_path(skills)

    for path_name, stats in sorted(path_completion.items(),
                                   key=lambda x: x[1]['percentage'],
                                   reverse=True):
        percentage = stats['percentage']
        completed = stats['completed']
        required = stats['required']

        status = "âœ…" if percentage == 100 else "âš ï¸" if percentage >= 60 else "âŒ"
        print(f"   {status} {path_name}: {completed}/{required} ({percentage:.0f}%)")

    # è¯­è¨€æ”¯æŒåˆ†æ
    print("\nğŸŒ è¯­è¨€æ”¯æŒåˆ†æ:")

    chinese_skills = sum(1 for s in skills if s['quality_analysis']['has_chinese'])
    english_skills = len(skills) - chinese_skills

    print(f"   ä¸­æ–‡æŠ€èƒ½: {chinese_skills} ä¸ª ({chinese_skills*100//len(skills)}%)")
    print(f"   è‹±æ–‡æŠ€èƒ½: {english_skills} ä¸ª ({english_skills*100//len(skills)}%)")

    # æ¨èæ”¹è¿›
    print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")

    if has_troubleshooting < len(skills):
        missing = len(skills) - has_troubleshooting
        print(f"   â€¢ å»ºè®®ä¸º {missing} ä¸ªæŠ€èƒ½æ·»åŠ æ•…éšœæ’é™¤ç« èŠ‚")

    if has_tools < len(skills):
        missing = len(skills) - has_tools
        print(f"   â€¢ å»ºè®®ä¸º {missing} ä¸ªæŠ€èƒ½æ·»åŠ å·¥å…·å’Œèµ„æºç« èŠ‚")

    low_utility = [s for s in skills if s['utility_score'] < 70]
    if low_utility:
        print(f"   â€¢ å»ºè®®æå‡ä»¥ä¸‹æŠ€èƒ½çš„å®ç”¨æ€§:")
        for skill in low_utility:
            name = skill['metadata'].get('name', 'Unknown')
            score = skill['utility_score']
            print(f"     - {name} ({score:.1f}/100)")

    # æ€»ç»“
    print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                     åˆ†æå®Œæˆ                                    â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")


def main():
    print("ğŸ” å¼€å§‹æ·±åº¦åˆ†æ SKILL.md æ•ˆæœ...\n")

    # è·å–é¡¹ç›®æ ¹ç›®å½•
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    skills_dir = project_root / "examples" / ".claude" / "skills"

    print(f"ğŸ“ åˆ†æç›®å½•: {skills_dir}\n")

    # åˆ†ææ‰€æœ‰æŠ€èƒ½
    skills = []
    for entry in skills_dir.iterdir():
        if entry.is_dir():
            skill = analyze_skill(entry)
            if skill:
                skills.append(skill)

    if not skills:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• SKILL.md æ–‡ä»¶")
        return 1

    # æ‰“å°åˆ†ææŠ¥å‘Š
    print_analysis_report(skills)

    # ä¿å­˜è¯¦ç»†åˆ†æç»“æœ
    report_path = Path("skill_analysis_report.json")
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(skills, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“„ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: {report_path}\n")

    return 0


if __name__ == "__main__":
    exit(main())
